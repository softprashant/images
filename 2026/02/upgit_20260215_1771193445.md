I'll create a powerful story-based explanation of **Implementing Application Insights** with emotional connections to help you remember.

---

# **The Story of Maya's Implementation Journey**

## **The Problem: Knowing vs Doing**

**Meet Maya**, a developer who just read about Application Insights. After seeing how it saved Alex's company $6.5M/month, Maya is convinced: "We NEED this!"

But there's a gap between **knowing** monitoring is important and **actually implementing** it.

**Maya's challenge**:
- Company has 15 applications (different technologies: .NET, Java, Node.js, React)
- Some apps are 10 years old (can't change code easily)
- Some are brand new (in active development)
- Boss gave Maya **one week** to "make monitoring happen"
- Maya has never implemented Application Insights before

**The overwhelming question**: "WHERE DO I EVEN START?"

Maya feels the pressure: **"If I mess this up, we won't have the visibility Alex had. We'll be flying blind."**

---

## **Day 1 Monday: The Panic and The Plan**

### **9:00 AM: Maya's Panic Attack**

Maya opens Azure Portal. Clicks "Create Application Insights." Gets an Instrumentation Key.

**Now what?**

Maya stares at 15 applications:
1. **Legacy ASP.NET app** (2015, running on old Windows Server, code is scary)
2. **Modern .NET Core API** (2024, actively developed, clean code)
3. **Java Spring Boot service** (2022, runs in Docker)
4. **Node.js Express backend** (2023, serverless on Azure Functions)
5. **React SPA frontend** (2024, user-facing website)
... and 10 more

**Maya's thought**: "How do I instrument 15 different applications in different languages in one week? This is impossible!"

Maya feels **overwhelmed, anxious, defeated** before even starting.

### **10:00 AM: The Mentor Appears**

Maya's senior colleague, **Carlos**, sees the panic: "Let me show you the three implementation approaches. Choose based on each app's situation."

Carlos draws on the whiteboard:

![diagram](https://raw.githubusercontent.com/softprashant/images/master/2026/02/upgit_20260215_1771193124.svg)

**Carlos explains**: "You don't need to do everything at once. Start simple, expand later."

**Maya's emotion shifts from panic to hope**: "Maybe this IS possible!"

---

## **Day 1 Afternoon: The Quick Win (Runtime Instrumentation)**

### **The Legacy App Problem**

**Target**: Old ASP.NET app running on Windows Server 2012. Last modified 3 years ago. Original developers gone. Nobody wants to touch the code.

**Maya's constraint**: Cannot modify code. Cannot redeploy.

**Carlos's advice**: "Perfect candidate for runtime instrumentation!"

### **The Implementation**

![diagram](./Information-2.svg)

**Step-by-step what Maya did**:

**2:00 PM**: RDP into production server (heart pounding - touching production is scary)

**2:05 PM**: Downloads "Application Insights Status Monitor 2" from Microsoft

**2:10 PM**: Runs installer
- Installer detects IIS
- Finds ASP.NET applications
- Shows: "Found: LegacyPayroll.Web on IIS"

**2:15 PM**: Enters Instrumentation Key from Azure Portal

**2:18 PM**: Clicks "Install"
- Installer configures IIS modules
- Adds HTTP module to intercept requests
- No code changes, no recompilation

**2:20 PM**: Restarts IIS (`iisreset`)

**2:21 PM**: Opens Azure Portal, refreshes Application Insights

**2:25 PM**: **TELEMETRY APPEARS!**

```
Requests: 45 in last 5 minutes
Avg Duration: 1,200ms
Dependencies: SQL Database, Redis Cache
Exceptions: 2 (NullReferenceException)
```

**Maya's emotion**: **Pure joy!** "IT'S WORKING! I didn't touch a single line of code and I have complete visibility!"

**Carlos**: "See? That's the power of runtime instrumentation. Zero code changes, full monitoring."

### **What Just Happened Behind the Scenes**

Runtime instrumentation works by **injecting monitoring into the IIS pipeline**:

![diagram](./Information-3.svg)

**Key insight**: The application code has no idea monitoring exists. Status Monitor sits between IIS and the application, silently observing everything.

**Maya's learning**: "When you can't touch code, runtime instrumentation is a lifesaver."

---

## **Day 2 Tuesday: SDK Integration (The Modern Way)**

### **The New Application**

**Target**: Brand new .NET Core Web API in active development. Maya's team writes this code daily.

**Maya's opportunity**: Full source code access. Can add custom telemetry for business events.

**Carlos**: "For actively developed apps, SDK integration gives you superpowers."

### **The Implementation**

![diagram](./Information-4.svg)

**Step-by-step what Maya did**:

**9:00 AM**: Open the .NET Core Web API project in Visual Studio

**9:05 AM**: Install NuGet package
```bash
dotnet add package Microsoft.ApplicationInsights.AspNetCore
```

**9:07 AM**: Modify `Program.cs` (one line!)
```csharp
builder.Services.AddApplicationInsightsTelemetry("YOUR_INSTRUMENTATION_KEY");
```

**9:10 AM**: Deploy to Azure App Service

**9:15 AM**: Open Azure Portal

**9:17 AM**: **BASIC TELEMETRY FLOWING!**
- Requests automatically tracked
- Dependencies automatically tracked
- Exceptions automatically captured

**Maya's emotion**: "That was... easy? Just ONE line of code?"

**Carlos**: "That's automatic instrumentation. But now you can add custom telemetry."

### **Adding Custom Business Telemetry**

**9:30 AM**: Maya adds business event tracking

**The business problem**: Track when users complete account setup (critical onboarding metric)

**Maya's code**:

```csharp
public class AccountController : ControllerBase
{
    private readonly TelemetryClient _telemetry;
    
    public AccountController(TelemetryClient telemetry)
    {
        _telemetry = telemetry;
    }
    
    [HttpPost("complete-setup")]
    public async Task<IActionResult> CompleteSetup(SetupRequest request)
    {
        // Business logic
        await _accountService.CompleteSetup(request);
        
        // CUSTOM TELEMETRY
        _telemetry.TrackEvent("AccountSetupCompleted",
            properties: new Dictionary<string, string> {
                {"AccountType", request.AccountType},
                {"ReferralSource", request.ReferralSource},
                {"Industry", request.Industry}
            },
            metrics: new Dictionary<string, double> {
                {"SetupDurationSeconds", request.SetupDuration},
                {"StepsCompleted", request.StepsCompleted}
            });
            
        return Ok();
    }
}
```

**10:00 AM**: Maya deploys the updated code

**10:15 AM**: A real user completes setup

**10:17 AM**: Maya queries Application Insights:

```kql
customEvents
| where name == "AccountSetupCompleted"
| where timestamp > ago(10m)
| project timestamp, 
    AccountType = tostring(customDimensions.AccountType),
    SetupDuration = todouble(customMeasurements.SetupDurationSeconds)
```

**Result**:
```
timestamp: 2026-02-15 10:15:23
AccountType: Premium
SetupDuration: 145 seconds
```

**Maya's emotion**: **Empowerment!** "I can track ANYTHING I want! Not just technical metrics, but BUSINESS metrics!"

**The revelation**: SDK integration = automatic telemetry + custom business insights

---

## **Day 3 Wednesday: Client-Side Monitoring (The User Experience)**

### **The Frontend Problem**

**Target**: React SPA that users interact with directly.

**Maya's realization**: "I'm monitoring the backend API, but what about the user experience? What if the frontend is slow even though the backend is fast?"

**Carlos**: "You need client-side monitoring. Backend tells you server performance. Frontend tells you user experience."

### **The Implementation**

**11:00 AM**: Maya opens the React application

**11:05 AM**: Installs npm package
```bash
npm install @microsoft/applicationinsights-web
```

**11:10 AM**: Creates `appInsights.js` configuration file:

```javascript
import { ApplicationInsights } from '@microsoft/applicationinsights-web';

const appInsights = new ApplicationInsights({
    config: {
        instrumentationKey: 'YOUR_KEY',
        enableAutoRouteTracking: true,  // Track SPA navigation
        enableCorsCorrelation: true,     // Correlate client + server
        enableRequestHeaderTracking: true,
        enableResponseHeaderTracking: true
    }
});

appInsights.loadAppInsights();
appInsights.trackPageView();

export default appInsights;
```

**11:20 AM**: Adds to main `App.jsx`:

```javascript
import appInsights from './appInsights';

function App() {
    // Track button clicks
    const handleCheckout = () => {
        appInsights.trackEvent('CheckoutButtonClicked', {
            itemsInCart: cartItems.length,
            cartValue: calculateTotal()
        });
        
        // Proceed with checkout
        navigateToCheckout();
    };
    
    return (
        <button onClick={handleCheckout}>
            Proceed to Checkout
        </button>
    );
}
```

**11:30 AM**: Maya deploys the updated frontend

**11:45 AM**: Maya opens the website in her browser

**11:46 AM**: Clicks around, adds items to cart, clicks checkout button

**11:50 AM**: Opens Azure Portal

**AMAZING RESULTS**:

```
Browser telemetry:
- Page View: /products (load time: 850ms)
- Page View: /products/laptop-15 (load time: 420ms)  
- Custom Event: CheckoutButtonClicked (itemsInCart: 3, cartValue: 1299.99)
- AJAX: POST /api/checkout (duration: 1,200ms, success)
- Page View: /checkout (load time: 650ms)
```

**But here's the MAGIC - End-to-End Correlation**:

![diagram](./Information-5.svg)

**Maya clicks one browser event in Azure Portal and sees**:
1. User clicked "Checkout" button at 11:46:23.100
2. Browser sent AJAX POST to /api/checkout at 11:46:23.150
3. Backend API received request at 11:46:23.160
4. Backend queried database at 11:46:23.180 (took 800ms)
5. Backend called payment API at 11:46:23.990 (took 350ms)
6. Backend returned response at 11:46:24.350 (total: 1,200ms)
7. Browser received response at 11:46:24.360
8. Browser rendered checkout page at 11:46:24.370

**Total user wait time**: 1,270ms from click to page load

**Maya's emotion**: **Awe!** "I can see the ENTIRE user journey from click to completion across frontend and backend! This is incredible!"

**Carlos**: "That's operation ID correlation. Client and server telemetry linked automatically."

---

## **Day 4 Thursday: The Availability Tests Crisis**

### **The 3 AM Outage Nobody Knew About**

**8:00 AM Thursday**: Maya arrives at work. Checks email. Sees from the operations team:

**Email subject**: "Customers reported website down at 3:15 AM"

**Email body**: "Multiple customers called support between 3:15 AM and 4:30 AM reporting they couldn't access the website. We checked logs at 8 AM and found the issue started at 3:00 AM. Cause: SSL certificate expired. Fixed at 4:30 AM by renewing certificate. **We discovered the problem from customers, not monitoring**."

**Maya's emotion**: **Embarrassment and guilt.** "I set up Application Insights to catch issues early. But we still found out from users? I failed."

**Carlos**: "You're monitoring the application. You're not monitoring availability. Different things."

### **The Solution: Availability Tests**

**9:00 AM**: Maya sets up availability monitoring

![diagram](./Information-6.svg)

**Maya's implementation**:

**In Azure Portal**:
1. Opens Application Insights
2. Clicks "Availability"
3. Clicks "+ Add Standard test"
4. Configures:
   - **Test name**: "Homepage Availability"
   - **URL**: https://mycompany.com
   - **Test frequency**: Every 5 minutes
   - **Test locations**: Select 5 locations (North America, Europe, Asia, Australia, South America)
   - **Success criteria**: Response code 200, timeout < 30 seconds
   - **Alert**: Alert if 2+ locations fail within 5 minutes

5. Creates alert action group:
   - SMS to on-call engineer
   - Email to operations team
   - Webhook to PagerDuty

**9:15 AM**: Test configured and active

**To prove it works**, Maya asks Carlos to temporarily block port 443 on the test website.

**9:20 AM**: Within 5 minutes, alerts fire:

```
⚠️ AVAILABILITY ALERT
Test: Homepage Availability
Status: FAILED
Failed Locations: Virginia (timeout), London (timeout), Tokyo (timeout)
Success Locations: Sydney (200 OK), Brazil (200 OK)

This indicates: Site accessible in Asia/Pacific but not Americas/Europe
Possible cause: Regional networking issue or geolocation blocking
```

Maya's phone buzzes with SMS. Email arrives. PagerDuty notification.

Carlos unblocks the port. Within 5 minutes: "✅ RESOLVED: Homepage Availability restored"

**Maya's emotion**: **Relief!** "If last night's SSL certificate expiration happened now, I'd be alerted within 5 minutes instead of finding out 1.5 hours later from customers."

### **Adding Multi-Step Availability Test**

**10:00 AM**: Maya realizes: "What if the homepage is up but checkout is broken?"

Maya creates a **multi-step web test**:

**Scenario to test**:
1. Load homepage
2. Click "Products"
3. Click a specific product
4. Click "Add to Cart"
5. Click "Checkout"
6. Verify checkout page loads

**Implementation options**:

**Option 1**: Use Azure Portal Test Creator (record browser actions)
**Option 2**: Write custom TrackAvailability code

Maya chooses Option 2 for flexibility:

```csharp
private async Task<bool> CheckoutFlowAvailabilityTest()
{
    using var httpClient = new HttpClient();
    
    try 
    {
        // Step 1: Homepage
        var homeResponse = await httpClient.GetAsync("https://mycompany.com");
        if (!homeResponse.IsSuccessStatusCode) return false;
        
        // Step 2: Product page
        var productResponse = await httpClient.GetAsync("https://mycompany.com/products/laptop");
        if (!productResponse.IsSuccessStatusCode) return false;
        
        // Step 3: Add to cart API
        var cartResponse = await httpClient.PostAsync("https://mycompany.com/api/cart/add", content);
        if (!cartResponse.IsSuccessStatusCode) return false;
        
        // Step 4: Checkout page
        var checkoutResponse = await httpClient.GetAsync("https://mycompany.com/checkout");
        if (!checkoutResponse.IsSuccessStatusCode) return false;
        
        return true;  // All steps succeeded
    }
    catch
    {
        return false;  // Any step failed
    }
}

// Run this as Azure Function every 5 minutes, report to Application Insights
```

**Result**: Maya now monitors not just "is the site up?" but "can users complete critical business transactions?"

---

## **Day 5 Friday: Live Metrics During Deployment**

### **The Deployment Day Stress**

**2:00 PM Friday**: Maya's team is deploying a major update to production.

**Changes include**:
- New payment processing logic
- Database schema changes
- Updated checkout UI

**Maya's anxiety**: "What if the deployment breaks something? How will I know immediately?"

**Carlos**: "Open Live Metrics Stream. Watch the deployment in real-time."

### **The Live Metrics Experience**

**2:05 PM**: Maya opens Azure Portal → Application Insights → Live Metrics

**What Maya sees on screen**:

![diagram](./Information-7.svg)

**All green. All healthy.** Refresh rate: 1 second.

**2:10 PM**: Deployment starts. Maya switches the deployment to 20% of servers (canary deployment).

**2:12 PM**: Updated servers start receiving traffic.

**LIVE METRICS SHOWS**:

```
⚠️ WARNING
Request Duration INCREASED:
Old servers: 1,200ms average
New servers: 3,500ms average (3× slower!)

Failed Requests INCREASED:
Old servers: 0/sec
New servers: 5/sec (8% failure rate)

Exceptions appearing in event stream:
❌ SqlException: Timeout expired (server01-new)
❌ SqlException: Timeout expired (server02-new)
❌ SqlException: Timeout expired (server01-new)
```

**Maya's heart drops**: "Something is wrong with the new version!"

**2:15 PM**: Maya yells to the team: "STOP THE DEPLOYMENT! Rollback the 20%!"

**2:17 PM**: Operations rolls back the 20% to the old version.

**2:19 PM**: Live Metrics shows all green again. Failures stop.

**Maya's emotion**: **Crisis averted!** Not relief yet, but no longer panicking. "We caught it affecting only 20% of users for 7 minutes instead of 100% of users for hours."

### **The Investigation**

**2:25 PM**: Maya looks at the failed requests from the new version.

**Discovery**: New database schema migration included a required index, but the index wasn't created before deploying application code. Result: queries that previously took 100ms now take 30 seconds (no index = full table scan). Queries timeout after 30 seconds.

**The fix**:
1. Create the missing database index
2. Verify queries are fast again
3. Redeploy application code
4. Watch Live Metrics - all green!
5. Complete deployment to 100%

**3:00 PM**: Deployment successful. No production impact to 80% of users. Only 20% saw slow performance for 7 minutes.

**Maya's emotion**: **Profound gratitude for Live Metrics.** "Without real-time visibility during deployment, we would have deployed to 100% of servers, broken production for everyone, spent an hour debugging, then rolled back. This saved us from a disaster."

**Carlos**: "This is why we always watch Live Metrics during deployments. Instant feedback."

---

## **The Five Implementation Approaches Compared**

### **Maya's Week Summary**

![diagram](./Information-8.svg)

### **Maya's Decision Matrix**

| Application           | Approach Used           | Reason                           | Time Taken |
| --------------------- | ----------------------- | -------------------------------- | ---------- |
| Legacy ASP.NET (2015) | Runtime Instrumentation | Can't modify old code            | 20 minutes |
| New .NET Core API     | SDK Integration         | Need custom events               | 30 minutes |
| React SPA             | JavaScript SDK          | Track user experience            | 40 minutes |
| Java Spring Boot      | Runtime (Java Agent)    | Quick start, expand later        | 25 minutes |
| Node.js Functions     | SDK Integration         | Serverless, need custom tracking | 35 minutes |
| Homepage Monitoring   | Availability Tests      | Ensure uptime 24/7               | 15 minutes |

**Total time for 6 monitoring implementations**: 2 hours 45 minutes

**Maya's realization**: "I thought it would take a week. It took less than 3 hours to get basic monitoring on everything. I can refine and add custom telemetry over the next weeks."

---

## **The Results: Two Weeks Later**

### **Monday Morning Metrics Review**

**Maya's dashboard shows**:

**Before Application Insights (blind)**:
- Unknown: How many users visited the site
- Unknown: Which features are slow
- Unknown: Where users abandoned checkout
- Unknown: When production went down at 3 AM
- **Mean Time to Detect (MTTD)**: 1.5 hours (discovered from customer complaints)
- **Mean Time to Repair (MTTR)**: 4 hours (finding root cause was slow)

**After Application Insights (visibility)**:
- **Daily active users**: 12,450
- **Slowest endpoint**: POST /api/recommendations (4.2s average - needs optimization)
- **Checkout abandonment**: 35% drop-off at payment step (UI issue identified)
- **3 AM SSL expiration**: Would have alerted within 5 minutes
- **MTTD**: 3 minutes (Smart Detection + Availability Tests)
- **MTTR**: 15 minutes (Live Metrics + KQL queries pinpoint issues immediately)

**Business impact discovered**:
- **Mobile users** experience 2.5× slower page loads than desktop (optimization opportunity)
- **Premium account signups** dropped 60% after recent UI change (revert needed)
- **Search feature** used by 78% of users but returns results in 3.8s (optimization priority #1)

### **Maya's Emotion: Transformation**

**Before (Monday Day 1)**: Overwhelmed, anxious, "This is impossible in one week!"

**After (Friday Day 5)**: Empowered, confident, "I have complete visibility across 15 applications!"

**The key learning**: "Implementation isn't one massive project. It's choosing the right approach for each application based on constraints."

---

## **The Three Pillars of Implementation**

### **Pillar 1: Start Simple, Expand Later**

![diagram](./Information-9.svg)

**Maya's approach**:
- **Week 1**: Get basic telemetry flowing (runtime instrumentation + SDK basics)
- **Week 2**: Add custom business events as you learn what matters
- **Week 3**: Build dashboards, configure alerts, optimize performance
- **Month 2+**: Continuous refinement based on insights

**Don't try to do everything Day 1**. Maya started with automatic instrumentation, added custom tracking where needed, then built dashboards.

### **Pillar 2: Match Approach to Constraint**

| Constraint                         | Solution                       |
| ---------------------------------- | ------------------------------ |
| Can't modify code                  | Runtime instrumentation        |
| Old application, risky to touch    | Runtime instrumentation        |
| Active development                 | SDK integration                |
| Need custom business events        | SDK integration (required)     |
| Need to understand user experience | JavaScript SDK (client-side)   |
| Need uptime monitoring             | Availability tests             |
| Multiple technology stacks         | Mix approaches per application |

**Maya's legacy app**: Runtime instrumentation (can't touch code)
**Maya's new app**: SDK integration (full control)
**Maya's frontend**: JavaScript SDK (user experience)
**Maya's uptime**: Availability tests (external monitoring)

### **Pillar 3: Monitor the Monitoring**

**Maya learned the hard way**: Set up monitoring, but verify it's working!

**Verification checklist**:
1. **Check data flow**: Query Application Insights to confirm telemetry is arriving
2. **Validate correlation**: Ensure frontend and backend events have same operation ID
3. **Test alerts**: Trigger test failures to confirm alerts fire
4. **Review latency**: Verify data appears within 5 minutes
5. **Check sampling**: Ensure sampling isn't hiding important events

**Maya's Friday routine** (5 minutes weekly):
```kql
// Verify all applications are reporting
Heartbeat
| summarize LastSeen = max(TimeGenerated) by Computer
| where LastSeen < ago(10m)
// Should return empty (all applications reporting recently)

// Check request volume looks normal
requests
| where timestamp > ago(24h)
| summarize RequestPerHour = count() by bin(timestamp, 1h)
| render timechart
// Should see normal daily pattern
```

---

## **Practical Example: The Java Application**

### **Challenge**: Java Spring Boot application running in Docker

**Maya's fear**: "I'm primarily a .NET developer. I don't know Java well. How do I instrument it?"

### **Solution: Java Agent (Runtime Instrumentation)**

**10:00 AM**: Maya downloads Application Insights Java Agent

**10:05 AM**: Adds one line to Dockerfile:

```dockerfile
FROM openjdk:11-jre
COPY target/myapp.jar /app/myapp.jar

# ADD THIS ONE LINE:
COPY applicationinsights-agent-3.4.jar /app/applicationinsights-agent.jar

ENV APPLICATIONINSIGHTS_CONNECTION_STRING="YOUR_CONNECTION_STRING"

ENTRYPOINT ["java", "-javaagent:/app/applicationinsights-agent.jar", "-jar", "/app/myapp.jar"]
```

**10:10 AM**: Rebuilds Docker image

**10:15 AM**: Deploys to Kubernetes cluster

**10:20 AM**: Checks Application Insights

**TELEMETRY FLOWING**:
- HTTP requests automatically captured
- Database calls (Hibernate) automatically tracked
- Redis cache calls automatically tracked
- Exceptions automatically captured

**Zero code changes in the Java application itself**. Just added Java agent to JVM startup.

**Maya's emotion**: **Amazement!** "I don't need to understand Java internals. The agent handles everything!"

### **Later: Adding Custom Events to Java**

**Week 2**: Maya wants to track custom business event in Java app.

**Simple code addition**:

```java
import com.microsoft.applicationinsights.TelemetryClient;

@RestController
public class OrderController {
    
    private TelemetryClient telemetry = new TelemetryClient();
    
    @PostMapping("/orders")
    public ResponseEntity createOrder(@RequestBody OrderRequest request) {
        // Business logic
        Order order = orderService.createOrder(request);
        
        // CUSTOM TELEMETRY
        Map<String, String> properties = new HashMap<>();
        properties.put("OrderType", request.getOrderType());
        properties.put("CustomerTier", request.getCustomerTier());
        
        Map<String, Double> metrics = new HashMap<>();
        metrics.put("OrderValue", order.getTotalValue());
        metrics.put("ItemCount", order.getItems().size());
        
        telemetry.trackEvent("OrderCreated", properties, metrics);
        
        return ResponseEntity.ok(order);
    }
}
```

**Result**: Business events tracked alongside automatic telemetry.

---

## **Real-World Scenario: The Mobile App**

### **Challenge**: iOS and Android mobile apps

**Maya realizes**: "We have mobile apps! How do I monitor those?"

**Carlos**: "Use Visual Studio App Center with Application Insights integration."

### **The Mobile Implementation**

**For iOS (Swift)**:

```swift
import AppCenter
import AppCenterAnalytics
import AppCenterCrashes

func application(_ application: UIApplication, 
                 didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?) -> Bool {
    
    AppCenter.start(withAppId: "YOUR_APP_CENTER_ID", 
                    services: [Analytics.self, Crashes.self])
    
    return true
}

// Track custom event
Analytics.trackEvent("ProductViewed", withProperties: [
    "ProductID": productId,
    "Category": category,
    "Price": price
])
```

**For Android (Kotlin)**:

```kotlin
import com.microsoft.appcenter.AppCenter
import com.microsoft.appcenter.analytics.Analytics
import com.microsoft.appcenter.crashes.Crashes

class MyApplication : Application() {
    override fun onCreate() {
        super.onCreate()
        AppCenter.start(
            application, "YOUR_APP_CENTER_ID",
            Analytics::class.java, Crashes::class.java
        )
    }
}

// Track custom event
val properties = HashMap<String, String>()
properties["ProductID"] = productId
properties["Category"] = category
Analytics.trackEvent("ProductViewed", properties)
```

**What Maya gets**:
- Crash reporting (automatic stack traces when app crashes)
- Analytics (sessions, events, user properties)
- Device information (OS version, manufacturer, screen size)
- Network conditions (WiFi vs cellular)
- **Telemetry flows to Application Insights** for unified analysis

**Maya can now query**:

```kql
customEvents
| where name == "ProductViewed"
| where client_Type == "Mobile"
| summarize Views = count() by tostring(customDimensions.Category)
| order by Views desc
```

**Result**: "Electronics category viewed 12,450 times on mobile (top category)"

---

## **Summary Sentence**

Application Insights implementation offers multiple approaches—runtime instrumentation for legacy applications requiring zero code changes through agent-based monitoring (install in minutes), SDK integration for modern applications enabling custom business event tracking and detailed telemetry control, JavaScript SDK for client-side user experience monitoring with automatic page view and AJAX tracking, availability tests for external uptime monitoring from global locations, and mobile SDK integration through App Center for crash reporting and analytics—allowing you to choose the right approach based on whether you can modify code, need custom telemetry, require immediate visibility, or must monitor across web, mobile, and backend platforms, ultimately enabling complete observability by starting simple with automatic instrumentation and expanding to custom business metrics as you learn what matters, transforming from blind production debugging to data-driven performance engineering through incremental implementation that matches your constraints and grows with your needs.

---

## **50+ Implementation Questions and Answers**

### **Approach Selection (Q1-Q10)**

**Q1: What are the three main approaches to implementing Application Insights?**
**A:** The three main approaches are: (1) **Runtime instrumentation** - install monitoring agent on the server without modifying application code, suitable for legacy or production applications you can't easily change; (2) **SDK integration** - add Application Insights SDK to your source code through NuGet/Maven/npm packages, providing full control and ability to add custom telemetry; (3) **Client-side monitoring** - embed JavaScript SDK in web pages or use mobile SDKs for user experience tracking. Runtime instrumentation takes 10-20 minutes but provides only automatic telemetry. SDK integration takes 30-60 minutes but enables custom business event tracking. Client-side adds user perspective that server-side monitoring misses.

**Q2: When should you use runtime instrumentation vs SDK integration?**
**A:** Use **runtime instrumentation** when: (1) You cannot modify application code (legacy applications, no access to source); (2) Application is in production and redeployment is risky; (3) You need monitoring immediately without development time; (4) Basic monitoring (requests, dependencies, exceptions) is sufficient. Use **SDK integration** when: (1) You have source code access and active development; (2) You need custom business events ("OrderCompleted", "UserRegistered"); (3) You want custom metrics (cart value, search relevance); (4) You need custom properties for segmentation (customer tier, A/B test variant). Maya used runtime instrumentation for the legacy payroll app (can't touch 3-year-old code) and SDK integration for the new .NET Core API (actively developed, needs custom tracking).

**Q3: How does runtime instrumentation work without code changes?**
**A:** Runtime instrumentation injects monitoring into the application hosting layer. For IIS applications, Status Monitor installs an HTTP module that intercepts every request at the IIS level before it reaches your application code. The module starts a timer when a request arrives, passes the request to your application (which processes normally, unaware monitoring exists), observes all outbound calls (database, APIs, caches), captures any exceptions, records the response time when your application returns the response, and sends all telemetry to Azure. Your application code runs exactly as before—the monitoring layer wraps around it transparently. For Java, the agent uses bytecode instrumentation to modify classes at JVM load time. For Node.js, the SDK uses monkey-patching to intercept built-in modules.

**Q4: What is the advantage of SDK integration over runtime instrumentation?**
**A:** SDK integration provides three major advantages: (1) **Custom telemetry** - track business-specific events (purchases, signups, feature usage) and metrics (order value, cart size, conversion rates) that runtime instrumentation can't capture; (2) **Full control** - configure sampling, filtering, telemetry processors, dependencies tracking, and correlation behavior; (3) **All platforms** - not limited to web applications, works with console apps, background workers, Azure Functions, batch processes. Runtime instrumentation only sees what flows through the web server. SDK integration tracks everything including background jobs, scheduled tasks, command-line utilities. Maya needed SDK integration for the .NET Core API to track "AccountSetupCompleted" events with business properties like account type and referral source—impossible with runtime instrumentation.

**Q5: Can you use both runtime instrumentation and SDK integration in the same application?**
**A:** No, using both causes duplicate telemetry and conflicts. Choose one approach per application. However, you can **transition** from runtime instrumentation to SDK integration: (1) Start with runtime instrumentation for immediate visibility; (2) Later, add SDK to source code for custom telemetry; (3) Remove/disable runtime instrumentation; (4) Redeploy with SDK only. Or use **different approaches for different applications**: legacy app uses runtime instrumentation (can't change code), new API uses SDK integration (have source code). This is what Maya did—mixed approaches across her 15 applications based on each application's constraints and needs.

**Q6: How long does it take to implement Application Insights?**
**A:** Implementation time varies by approach: **Runtime instrumentation**: 10-20 minutes (download agent, install, configure instrumentation key, restart service). **SDK integration (basic)**: 30 minutes (install package, add one line to startup code, deploy). **SDK integration (custom telemetry)**: 1-2 hours (basic setup + writing custom event tracking code). **Client-side JavaScript**: 30-40 minutes (install npm package, configure, add to HTML). **Availability tests**: 10-15 minutes (configure in Azure Portal). **Mobile apps**: 1-2 hours (App Center setup, SDK integration, test). Maya implemented basic monitoring across 6 different applications in under 3 hours. Comprehensive implementation with custom telemetry, dashboards, and alerts took her about 1 week total while still doing regular development work.

**Q7: What platforms and languages does Application Insights support?**
**A:** Application Insights supports extensive platforms: **Server-side**: ASP.NET (Framework and Core), Java (Spring Boot, Jakarta EE, Tomcat), Node.js (Express, Koa, Hapi), Python (Flask, Django), Go, Ruby, PHP. **Client-side**: JavaScript (vanilla and all frameworks), TypeScript, React, Angular, Vue. **Mobile**: iOS (Swift, Objective-C), Android (Java, Kotlin), React Native, Xamarin, Flutter. **Specialized**: Azure Functions, Logic Apps, Azure Web Jobs, Service Fabric, Kubernetes (via OpenTelemetry). If your language isn't directly supported, you can use the REST API to send custom telemetry or OpenTelemetry exporters. Maya had .NET, Java, Node.js, and React applications—all supported with official SDKs.

**Q8: Do you need different Application Insights resources for different applications?**
**A:** You have two organizational strategies: **Strategy 1 - Multiple resources**: Create separate Application Insights resource for each application or component. Advantages: isolation (testing doesn't pollute production data), independent retention policies, different access controls, simpler to understand which telemetry comes from which app. **Strategy 2 - Single shared resource**: All applications send to one resource using `cloud_RoleName` to differentiate. Advantages: unified Application Map showing how components interact, easier cross-component queries, simpler billing. Maya chose **one resource per environment** (separate for dev, staging, prod) but within production, all components (web, API, payment service) shared one resource using cloud_RoleName to differentiate. This provided environment isolation but enabled easy cross-component correlation within each environment.

**Q9: What's the difference between Instrumentation Key and Connection String?**
**A:** Both identify where telemetry should be sent: **Instrumentation Key** (older): GUID format like "a1b2c3d4-e5f6-g7h8-i9j0-k1l2m3n4o5p6", simple string, still widely supported. **Connection String** (newer, recommended): Full connection details including endpoint: "InstrumentationKey=xxx;IngestionEndpoint=https://region.in.applicationinsights.azure.com/...". Connection String is better because: (1) Explicitly specifies Azure region for data sovereignty; (2) Supports sovereign clouds (Azure Government, Azure China); (3) Allows custom ingestion endpoints. Both work, but new implementations should use Connection String. In code, both are configured the same way—just provide the string. Azure Portal shows both; copy whichever your SDK version requires.

**Q10: How do you handle multiple environments (dev, staging, production)?**
**A:** Create separate Application Insights resources per environment with different instrumentation keys/connection strings. Configure your application to use environment-specific keys: **Option 1 - Configuration files**: appsettings.Development.json has dev key, appsettings.Production.json has prod key. **Option 2 - Environment variables**: Read from environment variable at startup; deployment pipeline sets appropriate value per environment. **Option 3 - Azure App Configuration**: Centralize configuration, application fetches appropriate key based on environment name. **Benefits**: Production telemetry isn't contaminated by testing traffic, different retention (keep prod 90 days, dev 30 days), different access controls (more people see dev data, restricted prod access), different alerting (prod alerts immediately, dev doesn't). Maya created Application Insights resources named: "MyApp-Dev", "MyApp-Staging", "MyApp-Production" with different keys configured via Azure Pipeline variables during deployment.

### **Runtime Instrumentation Details (Q11-Q20)**

**Q11: What is Status Monitor and how do you use it?**
**A:** Status Monitor (now called "Application Insights Agent") is a tool for enabling Application Insights on IIS-hosted ASP.NET applications without code changes. **Installation steps**: (1) Download from Microsoft (ApplicationInsightsAgent.msi); (2) RDP to the Windows Server hosting IIS; (3) Run the installer executable; (4) Installer automatically detects IIS and finds ASP.NET applications; (5) Enter your Application Insights Instrumentation Key; (6) Installer configures IIS modules to intercept HTTP requests; (7) Restart IIS (command: `iisreset`). **What happens**: Status Monitor installs HTTP modules that wrap around your application, capturing every request, dependency call, and exception automatically. Maya used this on the legacy payroll application running on Windows Server 2012—total time from download to seeing telemetry: 21 minutes. No code changes, no recompilation, no risky deployment to production.

**Q12: Can runtime instrumentation monitor applications not hosted in IIS?**
**A:** Runtime instrumentation approaches vary by platform: **IIS applications**: Use Status Monitor/Application Insights Agent (Windows only). **Azure App Service**: Enable Application Insights directly in Azure Portal (one-click, any language). **Java applications**: Use Application Insights Java Agent (works with Tomcat, JBoss, WebLogic, standalone JARs—any JVM). **Node.js**: Runtime instrumentation limited; use SDK instead (npm package is easy enough). **Docker containers**: Mount Java agent or .NET agent as volume, configure via environment variables. **Azure Functions**: Enable via portal (built-in integration). Maya's Java Spring Boot application running in Docker used the Java agent approach: added one line to Dockerfile to include the agent JAR, set environment variable Connection String, and done—runtime instrumentation without modifying Java code.

**Q13: What telemetry does runtime instrumentation automatically capture?**
**A:** Runtime instrumentation provides comprehensive automatic telemetry: (1) **HTTP requests** - every incoming request with URL, method, response code, duration, timestamp; (2) **Dependencies** - every outbound call including SQL database (detects queries, connection, duration), HTTP/REST APIs (detects endpoint, method, response code), Redis cache (detects operations), message queues (Azure Service Bus, RabbitMQ); (3) **Exceptions** - unhandled exceptions with full stack trace and exception type; (4) **Performance counters** - CPU, memory, request queue length from the host server; (5) **Request/response headers** - if configured, captures HTTP headers for troubleshooting. **What's missing**: Custom business events (require SDK integration), custom metrics (require SDK), custom properties for segmentation (require SDK). Maya's runtime-instrumented legacy app showed all requests, SQL queries, and exceptions without any code—but couldn't track business events like "PayrollProcessed" (would require SDK integration).

**Q14: Does runtime instrumentation impact application performance?**
**A:** Performance impact is minimal, similar to SDK integration: **Typical overhead**: <1% CPU usage, 50-100 KB/sec network bandwidth. **Why so low**: (1) **Non-blocking** - telemetry collection doesn't slow request processing; (2) **Asynchronous** - data is sent in background thread after response returns to user; (3) **Batched** - multiple telemetry items sent together; (4) **Sampled** - high-traffic applications automatically reduce data volume while maintaining accuracy. **Maya's real-world test**: Legacy payroll application before Status Monitor averaged 850ms response time with 30% CPU. After Status Monitor: 855ms response time (+5ms, well within normal variance), 31% CPU (+1% overhead). Performance difference was unmeasurable to users. The monitoring layer is highly optimized to be invisible.

**Q15: Can you add custom telemetry after using runtime instrumentation?**
**A:** You have two options: **Option 1 - Stay with runtime instrumentation**: You cannot add custom telemetry. Runtime instrumentation provides only automatic telemetry (requests, dependencies, exceptions). If you need custom events, you must switch to SDK integration. **Option 2 - Transition to SDK integration**: (1) Add Application Insights SDK to your source code; (2) Disable/remove runtime instrumentation (uninstall Status Monitor or remove Java agent); (3) Redeploy application with SDK; (4) Now you can add custom telemetry. This is a one-way migration. Maya started with runtime instrumentation on the legacy payroll app for immediate visibility with zero code changes. Two months later, when the team had time to update the code, they added the SDK for custom telemetry tracking ("PayrollCycleCompleted" event) and removed Status Monitor.

**Q16: How do you enable Application Insights for Azure App Service without code?**
**A:** Azure App Service has built-in Application Insights integration (easiest runtime instrumentation): **Steps**: (1) Open Azure Portal → your App Service; (2) Click "Application Insights" in left menu; (3) Click "Turn on Application Insights"; (4) Select existing Application Insights resource or create new; (5) Click OK. **Result**: Azure automatically injects monitoring into your app without code changes or redeployment. Works for: ASP.NET, ASP.NET Core, Node.js, Java, Python, PHP. Behind the scenes, Azure adds environment variables and injects monitoring extensions. **Telemetry starts flowing in 5-10 minutes**. Maya enabled this on 3 Azure App Services in total time under 5 minutes by clicking through the portal. No code changes, no deployment, automatic instrumentation. This is the easiest implementation approach when hosting in Azure App Service.

**Q17: What is auto-instrumentation in Azure Functions?**
**A:** Azure Functions has Application Insights integration built into the runtime: **Automatic telemetry**: (1) Every function execution tracked as request; (2) Duration of function execution; (3) Success/failure status; (4) All dependencies (HTTP calls, database, storage) automatically tracked; (5) Exceptions automatically captured; (6) Logs written with ILogger automatically sent. **How to enable**: When creating Function App, specify Application Insights resource (checkbox in portal). For existing Function App, add Application Insights configuration in portal. **Custom telemetry**: Use dependency injection to get TelemetryClient and call TrackEvent/TrackMetric. Maya's Node.js Functions had monitoring enabled at creation time—every function invocation automatically tracked with full telemetry, zero configuration needed.

**Q18: Can runtime instrumentation monitor stored procedures and database internals?**
**A:** Runtime instrumentation sees **external perspective** only: (1) **What it tracks**: That your application called database, the SQL query text, how long the database took to respond, success or failure. (2) **What it doesn't track**: Internal database execution (query plan, lock waits, CPU usage inside database), which indexes were used, tempdb usage. For example, Maya's application calls `SELECT * FROM Orders WHERE OrderDate > @date`. Application Insights sees: "SQL query executed on 'mydb.database.windows.net', duration 1,200ms, returned 500 rows." But doesn't see that SQL Server did a full table scan because an index is missing. For deep database monitoring, use SQL-specific monitoring (Azure SQL Analytics, SQL Server Extended Events) alongside Application Insights.

**Q19: How do you troubleshoot when runtime instrumentation isn't working?**
**A:** Follow systematic troubleshooting: (1) **Verify installation**: Check that monitoring agent/extension is installed and running (for Status Monitor: check IIS modules list; for Java: verify -javaagent in JVM arguments); (2) **Check configuration**: Verify instrumentation key/connection string is correct (query Application Insights to confirm this key exists); (3) **Check network**: Can server reach `*.in.applicationinsights.azure.com`? Test with `Test-NetConnection` from PowerShell; (4) **Check agent status**: For Status Monitor: check Windows Services for "Application Insights Status Monitor"; for Java: check application startup logs for agent initialization; (5) **Generate test traffic**: Make requests to application and wait 10-15 minutes (latency); (6) **Query directly**: In Application Insights, run `requests | where timestamp > ago(30m)`. Maya's troubleshooting when Java agent "didn't work": checked logs, found "Connection String not set" error. Fixed environment variable, restarted container, telemetry appeared 5 minutes later.

**Q20: What's the difference between classic Status Monitor and Application Insights Agent?**
**A:** Both enable runtime instrumentation for IIS, but newer Application Insights Agent is better: **Classic Status Monitor** (older, being deprecated): Windows GUI application, requires interactive installation, limited to IIS on Windows Server, doesn't support .NET Core well. **Application Insights Agent** (current, recommended): PowerShell module-based, supports automation/scripting, works with .NET Core and .NET Framework, supports non-IIS scenarios (Azure App Service, VMs), stays up-to-date with agent improvements. **If you're starting new**, use Application Insights Agent. **If you have Status Monitor installed**, it still works but consider migrating. Installation command for new agent: `Install-Module -Name Az.ApplicationMonitor` then `Enable-ApplicationInsightsMonitoring`. Maya used Application Insights Agent for the 2015 ASP.NET app instead of classic Status Monitor because Agent supports better diagnostics and automation.

### **SDK Integration Details (Q21-Q30)**

**Q21: How do you add Application Insights SDK to .NET Core application?**
**A:** Three simple steps: **Step 1 - Install NuGet package**: Run `dotnet add package Microsoft.ApplicationInsights.AspNetCore` or use Visual Studio NuGet UI. **Step 2 - Configure in Program.cs**: Add one line: `builder.Services.AddApplicationInsightsTelemetry("YOUR_INSTRUMENTATION_KEY");` or better, read from configuration: `builder.Services.AddApplicationInsightsTelemetry(builder.Configuration["ApplicationInsights:ConnectionString"]);`. **Step 3 - Deploy**: Build and deploy your application. **Automatic features**: After these three steps, automatic telemetry flows: requests, dependencies, exceptions, performance counters. Maya completed this for the new .NET Core API in under 10 minutes. Then spent another 20 minutes adding custom events for business metrics ("AccountSetupCompleted", "SubscriptionUpgraded"). Total implementation time: 30 minutes for basic + custom monitoring.

**Q22: How do you add custom events in code?**
**A:** Use TelemetryClient to track custom events. **Step 1 - Get TelemetryClient via dependency injection**:
```csharp
public class OrderController : ControllerBase
{
    private readonly TelemetryClient _telemetry;
    
    public OrderController(TelemetryClient telemetry)
    {
        _telemetry = telemetry; // Injected automatically
    }
}
```
**Step 2 - Track events with properties and metrics**:
```csharp
_telemetry.TrackEvent("OrderCompleted",
    properties: new Dictionary<string, string> {
        {"PaymentMethod", "CreditCard"},
        {"ShippingMethod", "Express"},
        {"CustomerTier", "Premium"}
    },
    metrics: new Dictionary<string, double> {
        {"OrderValue", 299.99},
        {"ItemCount", 3},
        {"DiscountApplied", 15.00}
    });
```
**Result**: Event appears in customEvents table in Application Insights. Query: `customEvents | where name == "OrderCompleted" | project timestamp, customDimensions, customMeasurements`. Maya tracked 12 different business events tracking feature usage, conversion funnel steps, and business transactions.

**Q23: What's the difference between TrackEvent, TrackMetric, and TrackRequest?**
**A:** Three different telemetry types for different purposes: **TrackEvent** - discrete business actions ("UserRegistered", "VideoWatched", "DocumentDownloaded"). Use for: counting occurrences, segmenting bybehavior, understanding feature usage. **TrackMetric** - numeric measurements ("CartValue", "SearchResultCount", "QueueLength"). Use for: trending over time, capacity planning, business KPIs. **TrackRequest** - HTTP requests (automatically tracked by SDK, rarely called manually). Use for: custom protocols, non-HTTP requests, manual instrumentation. **Example**: Maya tracking checkout: `TrackEvent("CheckoutStarted")` when user clicks checkout (discrete action), `TrackMetric("CartValue", totalPrice)` to track cart value trend (numeric measurement), requests tracked automatically (don't need to track manually). Use events for "what happened", metrics for "how much".

**Q24: How do you track exceptions manually?**
**A:** Exceptions are automatically tracked when unhandled, but you can track handled exceptions manually:
```csharp
try
{
    await _paymentService.ProcessPayment(order);
}
catch (PaymentGatewayException ex)
{
    // Log the exception to Application Insights
    _telemetry.TrackException(ex, new Dictionary<string, string> {
        {"OrderId", order.Id.ToString()},
        {"PaymentMethod", order.PaymentMethod},
        {"Amount", order.TotalAmount.ToString()}
    });
    
    // Handle gracefully - return error to user
    return BadRequest("Payment failed. Please try again.");
}
```
**Why track handled exceptions?**: Even though you handled it, you want to know it's happening (maybe payment gateway is having issues, needs investigation). **Properties help correlation**: When many PaymentGatewayExceptions appear, you can group by PaymentMethod to see if one payment method has higher failure rate. Maya used this to discover PayPal integration had 15% failure rate (gateway timeouts) while credit card integration had only 0.5% failure rate (PayPal API was unreliable).

**Q25: How do you add custom properties to all telemetry?**
**A:** Use Telemetry Initializers to add properties globally: **Create initializer**:
```csharp
public class CustomPropertiesInitializer : ITelemetryInitializer
{
    public void Initialize(ITelemetry telemetry)
    {
        telemetry.Context.Properties["ApplicationVersion"] = "2.5.0";
        telemetry.Context.Properties["DeploymentRegion"] = "West US";
        telemetry.Context.Properties["Environment"] = "Production";
        
        // Add at request level
        if (telemetry is RequestTelemetry request)
        {
            request.Properties["ServerInstance"] = Environment.MachineName;
        }
    }
}
```
**Register in Program.cs**:
```csharp
builder.Services.AddSingleton<ITelemetryInitializer, CustomPropertiesInitializer>();
```
**Result**: Every piece of telemetry (requests, exceptions, events, metrics) automatically includes these properties. Query filtering: `requests | where customDimensions.Environment == "Production"`. Maya used this to add: application version (identify regressions after deployment), feature flags (segment users in A/B tests), customer tier (understand premium vs free user experience). These properties appear automatically on every telemetry item without manually adding to each TrackEvent call.

**Q26: How do you configure sampling in Application Insights SDK?**
**A:** Sampling reduces telemetry volume while maintaining statistical accuracy. **Adaptive sampling (recommended for most scenarios)**:
```csharp
builder.Services.AddApplicationInsightsTelemetry(options =>
{
    options.EnableAdaptiveSampling = true;
    options.AdaptiveSamplingMaxTelemetryItemsPerSecond = 5;
});
```
This collects 100% when traffic is low, automatically reduces to maintain ~5 items/second when traffic is high. **Fixed-rate sampling (predictable volume)**:
```csharp
builder.Services.Configure<ApplicationInsightsServiceOptions>(options =>
{
    options.EnableAdaptiveSampling = false;
});

builder.Services.AddSingleton<ITelemetryProcessor, SamplingTelemetryProcessor>(sp =>
{
    return new SamplingTelemetryProcessor(sp.GetService<ITelemetryProcessor>())
    {
        SamplingPercentage = 20 // Collect 20% of all requests
    };
});
```
**Exclusions** - never sample certain telemetry:
```csharp
processor.ExcludedTypes = "Exception"; // Always collect 100% of exceptions
```
Maya used adaptive sampling with 5 items/sec limit. During normal traffic (50 requests/minute), 100% collected. During Black Friday (5,000 requests/minute), sampling automatically reduced to 3% to stay within limits, saving 97% of costs while maintaining statistical visibility.

**Q27: How do you test Application Insights locally during development?**
**A:** Run application locally with Application Insights configured: **Option 1 - Use real Application Insights**: Configure with real instrumentation key, telemetry goes to Azure while developing. Advantage: see telemetry immediately in portal. Disadvantage: development noise in same workspace as testing. **Option 2 - Use local development key**: Create separate "Development" Application Insights resource, use its key locally. Advantage: separates dev traffic from test/prod. **Option 3 - Disable for local**: Set `ApplicationInsights:ConnectionString` to empty in local config, no telemetry sent. Advantage: fast local debugging without network overhead. Maya used Option 2: created "MyApp-Dev" Application Insights resource, configured local `appsettings.Development.json` with dev key. While coding locally, telemetry appeared in dev workspace. Before committing code, verified custom events were tracked correctly.

**Q28: How do you add Application Insights to Java Spring Boot?**
**A:** Two approaches for Java: runtime agent (covered earlier) or SDK integration. **SDK integration for custom events**: **Step 1 - Add Maven dependency**:
```xml
<dependency>
    <groupId>com.microsoft.azure</groupId>
    <artifactId>applicationinsights-spring-boot-starter</artifactId>
    <version>3.4.0</version>
</dependency>
```
**Step 2 - Configure connection string** in `application.properties`:
```properties
spring.application.insights.connection-string=YOUR_CONNECTION_STRING
```
**Step 3 - Auto-configuration** handles the rest (Spring Boot magic). **Step 4 - Track custom events**:
```java
@RestController
public class OrderController {
    
    @Autowired
    private TelemetryClient telemetry;
    
    @PostMapping("/orders")
    public ResponseEntity createOrder(@RequestBody Order order) {
        // Business logic
        orderService.create(order);
        
        // Track custom event
        telemetry.trackEvent("OrderCreated");
        
        return ResponseEntity.ok(order);
    }
}
```
Maya initially used Java agent for quick start (zero code), later added SDK dependency for custom business events when the Java team had time to enhance telemetry.

**Q29: How do you track background jobs and scheduled tasks?**
**A:** Background jobs need manual request tracking since there's no HTTP request to trigger automatic tracking:
```csharp
public class PayrollProcessor : BackgroundService
{
    private readonly TelemetryClient _telemetry;
    
    protected override async Task ExecuteAsync(CancellationToken stoppingToken)
    {
        while (!stoppingToken.IsCancellationRequested)
        {
            using (var operation = _telemetry.StartOperation<RequestTelemetry>("PayrollProcessor.Run"))
            {
                try
                {
                    await ProcessPayroll();
                    operation.Telemetry.Success = true;
                }
                catch (Exception ex)
                {
                    _telemetry.TrackException(ex);
                    operation.Telemetry.Success = false;
                    throw;
                }
            }
            
            await Task.Delay(TimeSpan.FromHours(24), stoppingToken);
        }
    }
}
```
**StartOperation creates**: request telemetry for the background job, operation ID that correlates all telemetry during execution, automatic duration tracking. **Result**: Background job appears in Application Insights as "request" (even though it's not HTTP), exceptions are correlated, dependencies during job execution are tracked, you can monitor: how long payroll processing takes, if it succeeded or failed, any exceptions that occurred. Maya tracked 5 background jobs this way: nightly report generation, data cleanup, email sending, cache warming, log archiving.

**Q30: How do you disable Application Insights for specific endpoints?**
**A:** Use telemetry processors to filter out unwanted telemetry:
```csharp
public class FilterHealthCheckTelemetry : ITelemetryProcessor
{
    private ITelemetryProcessor Next { get; set; }
    
    public FilterHealthCheckTelemetry(ITelemetryProcessor next)
    {
        Next = next;
    }
    
    public void Process(ITelemetry item)
    {
        if (item is RequestTelemetry request)
        {
            // Don't track health check endpoints
            if (request.Url.PathAndQuery.Contains("/health") || 
                request.Url.PathAndQuery.Contains("/readiness"))
            {
                return; // Don't send to Application Insights
            }
        }
        
        Next.Process(item); // Send everything else
    }
}

// Register
builder.Services.AddApplicationInsightsTelemetryProcessor<FilterHealthCheckTelemetry>();
```
**Why filter**: Health checks run every 5 seconds from load balancer (17,280 requests/day per server), pollute telemetry with non-user traffic, increase costs without value. Maya filtered: `/health`, `/readiness`, `/metrics` (Prometheus), `/favicon.ico`. Result: cleaner telemetry focusing on real user requests, lower costs (eliminated 30% of request volume that was just internal health checks).

### **Client-Side and Mobile (Q31-Q40)**

**Q31: How do you add JavaScript SDK to a web application?**
**A:** Two installation methods: **Method 1 - NPM (recommended for SPAs)**:
```bash
npm install --save @microsoft/applicationinsights-web
```
```javascript
import { ApplicationInsights } from '@microsoft/applicationinsights-web';

const appInsights = new ApplicationInsights({
    config: {
        connectionString: 'YOUR_CONNECTION_STRING',
        enableAutoRouteTracking: true  // For SPAs
    }
});

appInsights.loadAppInsights();
appInsights.trackPageView(); // Manually track SPA route changes
```
**Method 2 - CDN script tag (traditional websites)**:
```html
<script type="text/javascript">
!function(cfg){function e(){cfg.onInit&&cfg.onInit(i)}var...
<script>
  window.appInsights.config.connectionString = "YOUR_CONNECTION_STRING";
</script>
```
**What you get automatically**: page views, AJAX calls, client-side exceptions, page load performance. Maya used npm approach for React SPA, CDN approach for legacy multi-page site (easier to add script tag to master page

 template than rebuild entire frontend).

**Q32: How do you track custom events in JavaScript?**
**A:** Use trackEvent API similar to server-side:
```javascript
import appInsights from './appInsights';

function handleAddToCart(product) {
    appInsights.trackEvent('AddToCart', {
        productId: product.id,
        productName: product.name,
        category: product.category,
        price: product.price
    });
    
    // Continue with business logic
    cart.addItem(product);
}

function handleCheckout() {
    appInsights.trackEvent('CheckoutStarted', {
        itemCount: cart.items.length,
        cartValue: cart.total
    });
    
    navigateToCheckout();
}
```
**Result**: Events appear in customEvents table in Application Insights with client_Type="Browser". Query: `customEvents | where client_Type == "Browser" | where name == "AddToCart"`. Maya tracked: button clicks, form submissions, video plays, document downloads, search queries, feature interactions. This revealed 78% of users used the search feature (high adoption), but only 12% of searches resulted in clicks (poor relevance—needed algorithm improvement).

**Q33: How do you correlate client-side and server-side telemetry?**
**A:** Operation IDs automatically correlate frontend and backend telemetry when properly configured: **Step 1 - Enable CORS correlation in JavaScript SDK**:
```javascript
const appInsights = new ApplicationInsights({
    config: {
        connectionString: 'YOUR_CONNECTION_STRING',
        enableCorsCorrelation: true,  // CRITICAL!
        enableRequestHeaderTracking: true,
        enableResponseHeaderTracking: true,
        correlationHeaderExcludedDomains: ['*.google.com'] // Don't correlate external domains
    }
});
```
**Step 2 - Server must send CORS headers**: ASP.NET Core automatically handles this when Application Insights SDK is configured. **Result**: When browser makes AJAX call to backend, JavaScript SDK adds request headers (Request-Id, Request-Context), backend SDK reads these headers and uses same operation ID, all telemetry (client event → AJAX call → backend request → database call → response) shares operation ID. Maya validated correlation by querying:
```kql
let frontendEvent = customEvents | where name == "CheckoutStarted" | take 1;
union frontendEvent,
    (requests | where operation_Id == toscalar(frontendEvent | project operation_Id)),
    (dependencies | where operation_Id == toscalar(frontendEvent | project operation_Id))
| order by timestamp asc
```
Result showed complete transaction: user clicked button in browser, AJAX call sent, backend API processed, database queried, response returned—all linked by one operation ID.

**Q34: How do you track page load performance?**
**A:** JavaScript SDK automatically tracks page load metrics: **Automatic metrics collected**: (1) **Page load time** - from navigation start to page load complete; (2) **DNS lookup time** - time to resolve domain name; (3) **Network time** - time to download page; (4) **Server response time** - time server took to generate HTML; (5) **DOM processing time** - time to parse and build DOM; (6) **Time to Interactive (TTI)** - when page becomes usable. **Query in Application Insights**:
```kql
pageViews
| where timestamp > ago(24h)
| where name == "Checkout"
| summarize 
    AvgLoadTime = avg(duration),
    P95LoadTime = percentile(duration, 95),
    Count = count()
    by client_Type
| order by AvgLoadTime desc
```
Maya discovered: desktop users averaged 1.2s page load, mobile users averaged 4.8s page load (4× slower!). Drill-down revealed mobile users downloading 2 MB of images not optimized for mobile. Fix: implement responsive images (srcset), reduce mobile page load to 1.8s, mobile conversion increased 35%.

**Q35: How do you implement Application Insights in React applications?**
**A:** Use the React plugin for tight integration:
```bash
npm install @microsoft/applicationinsights-react-js
npm install @microsoft/applicationinsights-web
```
```jsx
import React from 'react';
import { ApplicationInsights } from '@microsoft/applicationinsights-web';
import { ReactPlugin } from '@microsoft/applicationinsights-react-js';
import { createBrowserHistory } from 'history';

const browserHistory = createBrowserHistory();
const reactPlugin = new ReactPlugin();

const appInsights = new ApplicationInsights({
    config: {
        connectionString: 'YOUR_CONNECTION_STRING',
        extensions: [reactPlugin],
        extensionConfig: {
            [reactPlugin.identifier]: { history: browserHistory }
        }
    }
});

appInsights.loadAppInsights();

// Wrap App with telemetry context
import { AppInsightsContext } from '@microsoft/applicationinsights-react-js';

function App() {
    return (
        <AppInsightsContext.Provider value={reactPlugin}>
            <Router history={browserHistory}>
                {/* Your app */}
            </Router>
        </AppInsightsContext.Provider>
    );
}
```
**Benefits**: Automatic route change tracking (SPA navigation), React component error boundaries tracked as exceptions, hooks for tracking custom events. Maya's React checkout flow automatically tracked: route navigation (/products → /cart → /checkout), React component errors, button clicks via custom hooks.

**Q36: How do you monitor mobile applications?**
**A:** Use Visual Studio App Center with Application Insights integration: **Setup**: (1) Create App Center project for iOS and Android apps; (2) Enable Application Insights connection in App Center (links App Center to Application Insights); (3) Add App Center SDK to mobile apps. **For iOS**:
```swift
import AppCenter
import AppCenterAnalytics
import AppCenterCrashes

AppCenter.start(withAppId: "YOUR_APP_CENTER_ID", 
                services: [Analytics.self, Crashes.self])
```
**For Android**:
```kotlin
AppCenter.start(application, "YOUR_APP_CENTER_ID",
                Analytics::class.java, Crashes::class.java)
```
**What you get**: Crash reporting (automatic stack traces when app crashes), analytics (sessions, custom events, user properties), device info (OS version, manufacturer, screen size, language), network conditions (WiFi vs cellular). **Telemetry flows to Application Insights**, query alongside web telemetry:
```kql
customEvents
| where timestamp > ago(7d)
| summarize EventCount = count() by client_Type
// Result: Browser: 45,000, Mobile-iOS: 12,000, Mobile-Android: 18,000
```
Maya discovered 40% of application usage was mobile, but mobile app had 3× higher crash rate than web (mobile app needed stability work).

**Q37: How do you track crashes in mobile applications?**
**A:** App Center automatically captures crashes: **What's captured**: Full stack trace, exception type and message, device information (model, OS version, memory, disk space), user information (anonymous ID unless you set user ID), breadcrumbs (last actions before crash), attachments (logs, screenshots if configured). **Query in Application Insights**:
```kql
exceptions
| where client_Type startswith "Mobile"
| where timestamp > ago(7d)
| summarize 
    CrashCount = count(),
    AffectedUsers = dcount(user_Id)
    by type, outerMessage, client_OS
| order by CrashCount desc
```
Maya found: iOS app had NullReferenceException in PaymentViewController affecting 450 users/week (15% of iOS users experienced crash during payment), caused by race condition when network was slow. Fix deployed, crash rate dropped from 15% to 0.2%.

**Q38: How do you track custom user properties in mobile apps?**
**A:** Set user properties that appear in all telemetry:
```swift
// iOS
AppCenter.setUserId("user123")
Analytics.trackEvent("ProductViewed", withProperties: [
    "ProductID": "laptop-15",
    "Category": "Electronics",
    "UserTier": "Premium"
])
```
```kotlin
// Android
AppCenter.setUserId("user123")
val properties = hashMapOf(
    "ProductID" to "laptop-15",
    "Category" to "Electronics",
    "UserTier" to "Premium"
)
Analytics.trackEvent("ProductViewed", properties)
```
**Segmentation**: Query by user properties:
```kql
customEvents
| where name == "PurchaseCompleted"
| where client_Type startswith "Mobile"
| extend UserTier = tostring(customDimensions.UserTier)
| summarize 
    Purchases = count(),
    AvgOrderValue = avg(todouble(customMeasurements.OrderValue))
    by UserTier
```
Result: Premium users on mobile had $340 average order value, Free users had $85 average order value. Premium users are 4× more valuable—optimize mobile experience for premium tier first.

**Q39: Can you use Application Insights without Visual Studio or Azure DevOps?**
**A:** Yes, Application Insights is platform and tool-independent: **What you need**: Azure subscription (for Application Insights resource), instrumentation key/connection string (from Azure Portal), SDK for your language (NPM, Maven, NuGet, pip, etc.). **What you DON'T need**: Visual Studio (can use VS Code, IntelliJ, Eclipse, vim, any editor), Azure DevOps (can use GitHub Actions, GitLab CI, Jenkins, any CI/CD), Windows (works on Linux, macOS). Maya's team used: VS Code (not Visual Studio), GitHub (not Azure DevOps), macOS for development (not Windows), Linux containers for hosting (not Windows Server). All worked perfectly with Application Insights. The only Microsoft product required is Azure itself for hosting the Application Insights service.

**Q40: How do you protect sensitive data from being sent to Application Insights?**
**A:** Use telemetry processors and configuration to filter sensitive data: **Filter request headers**:
```csharp
public class RemoveSensitiveDataProcessor : ITelemetryProcessor
{
    public void Process(ITelemetry item)
    {
        if (item is RequestTelemetry request)
        {
            // Remove authorization headers
            if (request.Properties.ContainsKey("Authorization"))
                request.Properties.Remove("Authorization");
                
            // Redact credit card numbers from URL
            request.Url = new Uri(RedactCreditCard(request.Url.ToString()));
        }
        
        Next.Process(item);
    }
    
    private string RedactCreditCard(string input)
    {
        return Regex.Replace(input, @"\d{4}-\d{4}-\d{4}-\d{4}", "XXXX-XXXX-XXXX-XXXX");
    }
}
```
**Disable specific data collection**:
```csharp
builder.Services.AddApplicationInsightsTelemetry(options =>
{
    options.EnableRequestTrackingTelemetryModule = true;
    options.EnableDependencyTrackingTelemetryModule = true;
    options.EnablePerformanceCounterCollectionModule = false; // Disable if sensitive
    options.EnableEventCounterCollectionModule = true;
    options.RequestCollectionOptions.TrackExceptions = true;
});
```
**IP address anonymization**: Set `DisableIpMasking = false` (default) to mask last octet of IP addresses. Maya configured: removed Authorization headers, redacted email addresses from URLs, masked full IP addresses (store only country/city for geographic analysis), never tracked credit card numbers or passwords. Result: comprehensive telemetry without PII risk.

### **Availability and Live Monitoring (Q41-Q50)**

**Q41: What are availability tests and why are they important?**
**A:** Availability tests are automated external checks that monitor if your application is accessible and responsive from different global locations. **Why important**: Application monitoring (SDK/agent) tells you if your application is running correctly *from inside the application*. But if your entire site is down (DNS failure, network issue, SSL certificate expired), internal monitoring can't send telemetry because it can't reach Azure. Availability tests run *externally from Azure data centers*, testing from user perspective. Maya's 3 AM SSL certificate expiration was invisible toApplication Insights SDK (server couldn't send telemetry because users couldn't connect). But availability tests would have alerted within 5 minutes because tests run from outside, independent of the application itself.

**Q42: What are the three types of availability tests?**
**A:** Three test types for different scenarios: (1) **URL ping test** - simplest, just checks if endpoint returns 200 OK within timeout. Configure: URL, locations, frequency, success criteria. Use for: basic "is site up?" monitoring. (2) **Multi-step web test** - sequence of HTTP requests simulating user workflow. Configure: upload .webtest file (record in Visual Studio or write XML). Use for: critical business transactions (login → add to cart → checkout). (3) **Custom TrackAvailability** - write your own code (Azure Function, console app) that tests availability and reports results. Use for: complex scenarios, APIs requiring authentication, testing internal endpoints. Maya used: URL ping tests for homepage and API health endpoint (simple, 5 minutes to configure), multi-step test for checkout flow (30 minutes to create), custom code for internal admin portal (requires VPN access, external tests couldn't reach it).

**Q43: How do you configure a URL availability test?**
**A:** Configure in Azure Portal in 5-10 minutes: **Steps**: (1) Open Application Insights resource; (2) Click "Availability" in left menu; (3) Click "+ Add Standard test"; (4) **Configure test**: Test name: "Homepage Availability", URL: https://mycompany.com, Test frequency: 5 minutes, Test locations: Select 5+ locations (North America, Europe, Asia, Australia, South America), Parse dependent requests: No (faster), Enable retries: Yes, Success criteria: Response code is 200, Response timeout < 30 seconds. (5) **Configure alerts**: Alert if 2+ locations fail within 5 minutes (prevents false alerts from single location network blip). (6) **Action group**: SMS to on-call, email to team, webhook to PagerDuty. (7) Click Create. **Result**: Test runs every 5 minutes from each location (5 locations × 12 times/hour = 60 tests/hour), alerts if 2+ consecutive failures from 2+ different locations. Maya configured 3 tests in 15 minutes: homepage, API, checkout page.

**Q44: How do you create a multi-step availability test?**
**A:** Multi-step tests require Visual Studio to record or manual XML creation: **Option 1 - Record in Visual Studio** (easier): (1) Open Visual Studio; (2) Create new Web Performance Test project; (3) Click "Add Recording"; (4) Browse your website (Visual Studio records every request); (5) Stop recording; (6) Right-click test, "Generate Web Test Plugin"; (7) Export as .webtest file; (8) Upload to Azure Portal. **Option 2 - Write XML manually** (more control):
```xml
<WebTest xmlns="http://microsoft.com/schemas/VisualStudio/TeamTest/2010">
  <Items>
    <Request Method="GET" Url="https://mycompany.com/products" />
    <Request Method="POST" Url="https://mycompany.com/api/cart/add">
      <Body>{"productId": "12345", "quantity": 1}</Body>
    </Request>
    <Request Method="GET" Url="https://mycompany.com/checkout" />
  </Items>
</WebTest>
```
**Upload**: Azure Portal → Availability → Add Classic test → Upload .webtest file. **Result**: Tests complete workflow end-to-end every 5 minutes, alerts if any step fails. Maya's checkout flow test: (1) Load homepage, (2) GET /products, (3) GET /products/laptop-15, (4) POST /api/cart/add, (5) GET /checkout, (6) Verify checkout page contains "Payment" text. Total test time: 3.5 seconds. Any step failing triggers alert.

**Q45: What is Live Metrics Stream and when should you use it?**
**A:** Live Metrics Stream shows real-time telemetry with 1-second refresh, near-zero latency (not the usual 5-minute delay). **What you see**: Request rate per second, failed requests per second, exception rate, average response time, dependency call durations, CPU and memory per server instance, live event stream showing individual requests/exceptions as they occur. **When to use**: (1) **During deployments** - watch performance degrade immediately if deployment has issues; (2) **During load testing** - see real-time impact of load on system; (3) **During incidents** - watch issue unfold in real-time; (4) **After configuration changes** - verify change had desired effect. **When NOT to use**: Historical analysis (only shows last 60 seconds), requires keeping browser tab open (not alerting). Maya kept Live Metrics Stream open during every production deployment (10-minute canary phase), watching request duration, failure rate, and exception stream. Caught 3 bad deployments before they reached all users.

**Q46: How do you use Live Metrics Stream to validate a deployment?**
**A:** Follow deployment validation workflow: **Pre-deployment**: (1) Open Live Metrics Stream 5 minutes before deployment, establish baseline: request rate ~45/sec, avg duration 1.2s, failure rate 0%, CPU 35%, memory 2.1 GB. (2) Take screenshot for comparison. **During deployment**: (3) Deploy to 10-20% of servers (canary); (4) Watch Live Metrics—compare canary servers vs stable servers (Live Metrics shows per-instance metrics); (5) **Green flags**: Request duration unchanged, failure rate remains 0%, exception stream empty, CPU similar. **Red flags**: Request duration increases 50%+, failures appearing, exceptions in event stream, memory growing rapidly. **Post-deployment decision**: After 10-15 minutes canary monitoring: if metrics are green, deploy to remaining servers; if metrics are red, rollback canary immediately, investigate before trying again. Maya's deployment process: canary 20% → watch 15 minutes → deploy remaining 80%, caught issues affecting only 20% of users for 15 minutes instead of 100% of users for hours.

**Q47: Can you customize what appears in Live Metrics Stream?**
**A:** Yes, add custom metrics and events that appear in real-time: **Add custom metric to Live Metrics**:
```csharp
var myMetric = new MetricConfiguration("OrdersPerMinute", MetricConfigurations.MetricNamespace);
var metric = _telemetry.GetMetric(myMetric);
metric.TrackValue(ordersProcessedThisMinute);
```
**Filter Live Metrics** in portal: Click "Filter" button, add filters like: `Request.Success == false` (show only failures), `Cloud.RoleName == "ProductAPI"` (show only specific service), `Dependency.Type == "SQL"` (show only database calls). **Custom event visibility**: Events tracked with `TrackEvent()` appear in Live Metrics event stream immediately (not just in historical queries). Maya added custom metric "CheckoutCompletionsPerMinute" that appeared in Live Metrics dashboard, tracked deployment impact on conversion rate in real-time, saw checkout rate drop from 25/minute to 5/minute within 2 minutes of bad deployment (immediate rollback signal).

**Q48: How do availability test results correlate with actual application telemetry?**
**A:** Availability tests create their own telemetry visible in Application Insights: **availabilityResults table** stores all test results:
```kql
availabilityResults
| where timestamp > ago(24h)
| where name == "Homepage Availability"
| summarize 
    SuccessRate = todouble(countif(success == true)) / count() * 100,
    AvgDuration = avg(duration),
    FailureCount = countif(success == false)
    by location
| order by FailureCount desc
```
**Correlation with app telemetry**: Availability test failures don't have operation IDs linking to application requests (they're external synthetic tests, not real user traffic). But you can correlate by time: if availability test failed 10:15 AM, query application telemetry near that time for errors:
```kql
union
    (availabilityResults | where timestamp between (datetime(10:10) .. datetime(10:20)) | where success == false),
    (exceptions | where timestamp between (datetime(10:10) .. datetime(10:20)))
| order by timestamp asc
```
Maya used this to investigate availability test failure at 3:15 AM: test showed "timeout", application exceptions showed "SqlException: transaction deadlock", correlating external symptom (timeout) with internal cause (database deadlock).

**Q49: What's the difference between availability monitoring and synthetic monitoring?**
**A:** They're the same concept with different names: **Availability monitoring** = **Synthetic monitoring** = automated tests simulating user behavior from external locations. **"Availability"** emphasizes uptime checking ("is it up?"). **"Synthetic"** emphasizes artificial traffic generation ("fake users testing the app"). Both describe: tests running from Azure data centers worldwide, testing at regular intervals (5 minutes typical), simulating user actions (load page, click button, submit form), alerting when tests fail. **Contrasted with real user monitoring (RUM)**: Synthetic monitoring uses fake/scripted traffic from Azure, RUM uses actual user browsers sending telemetry (what JavaScript SDK provides). Maya used both: synthetic monitoring (availability tests every 5 minutes checking uptime) and RUM (JavaScript SDK tracking real user experience including page load performance and actual usage patterns).

**Q50: How do you monitor Application Insights costs and optimize spending?**
**A:** Track and optimize Application Insights costs: **View costs**: Azure Portal → Cost Management → Cost Analysis → Group by Resource → Find your Application Insights resources. Shows GB ingested per day and cost. **Query data volume**:
```kql
Usage
| where TimeGenerated > ago(30d)
| where IsBillable == true
| summarize IngestedGB = sum(Quantity) / 1024 by DataType
| order by IngestedGB desc
```
**Result shows which telemetry types cost the most**: requests, dependencies, traces, exceptions, pageViews, customEvents. **Optimization strategies**: (1) **Enable sampling** - reduce to 10-20% for high-traffic apps; (2) **Filter health checks** - exclude `/health`, `/readiness` endpoints; (3) **Reduce trace logging** - only log warnings+ in production, not verbose; (4) **Limit custom events** - track business-critical events only, not every button click; (5) **Use commitment tiers** - commit to daily volume for 30% discount; (6) **Shorter retention** - 30 days adequate for operational data (not 90 days). Maya reduced costs 85% ($690/month → $105/month) by: enabling adaptive sampling (20% collection), filtering health checks (eliminated 40% of requests), reducing trace verbosity (Info → Warning level), using 100 GB/day commitment tier. Kept full visibility into issues while dramatically reducing costs.

---

## **Memory Reinforcement: Maya's Journey**

**Close your eyes and visualize Maya's emotional journey**:

1. **Monday morning panic**: "15 applications, 1 week, HOW?" Feel her overwhelm.

2. **Monday afternoon relief**: Legacy app monitoring in 21 minutes without touching code. "It IS possible!"

3. **Tuesday confidence**: Adding SDK to new app, tracking custom events, "I control what I measure!"

4. **Wednesday revelation**: Client-side monitoring showing complete user journey, "I see EVERYTHING!"

5. **Thursday embarrassment then resolution**: 3 AM outage discovered from customers, configure availability tests, "Never again!"

6. **Friday triumph**: Live Metrics catches bad deployment affecting only 20% for 15 minutes, "Monitoring saved us!"

---

**The One Sentence to Remember**: "Choose runtime instrumentation when you can't change code, SDK integration when you need custom telemetry, JavaScript SDK for user experience, and availability tests for uptime—implement incrementally based on constraints rather than trying to do everything at once."

Implementing Application Insights requires choosing among runtime instrumentation for codeless monitoring through agent installation taking 10-20 minutes per application, SDK integration for comprehensive monitoring with custom business event tracking taking 30-60 minutes per application, client-side JavaScript SDK for user experience monitoring including page load times and client-side errors, availability tests for external uptime monitoring from global Azure data centers, and mobile SDK integration through Visual Studio App Center for crash reporting and analytics—enabling you to match implementation approach to

 each application's constraints whether legacy code you can't modify, active development where custom telemetry adds value, user-facing frontends requiring experience monitoring, or critical endpoints needing 24/7 uptime verification—ultimately transforming from hoping applications work in production to knowing they work through data-driven visibility that you build incrementally starting simple with automatic instrumentation and expanding to custom business metrics as you learn what matters, allowing Maya to instrument 15 diverse applications in under one week by choosing the right approach for each rather than attempting one-size-fits-all implementation.

Similar code found with 1 license type
